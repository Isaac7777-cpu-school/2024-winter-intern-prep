# Transformer
这部分是 Transformer 相关资料，如果对 Transformer 模型很了解可以跳过
- Transformer原始论文导读：https://www.bilibili.com/video/BV1pu411o7BE
- GPT系列论文导读：https://www.bilibili.com/video/BV1AF411b7xQ/
- gpt2 的可复现仓库：https://github.com/karpathy/nanoGPT
- llama && OLMo，我们常使用 OLMo 做研究，因为训练过程开源：https://github.com/allenai/OLMo/tree/main/olmo
- 可以跟随 https://huggingface.co/learn 的教程实践

# Pytorch
我们使用 Pytorch 框架，为其编写 CUDA 代码实现自定义算子
- 熟悉 Pytorch 可以自己实现 nanoGPT
- 为 Pytorch 编写拓展算子：https://pytorch.org/tutorials/advanced/cpp_extension.html

# CUDA
- CUDA 比较偏向高性能领域，优化内容很杂，目前有系列视频教程：https://www.youtube.com/@GPUMODE/videos
- 推荐结合 Openai triton 使用，https://github.com/triton-lang/triton Triton 将 SM 内部的代码简化，可以从 SM level 理解并行编程。
- 我们主要使用 cutlass 模板库辅助编程:https://github.com/NVIDIA/cutlass, 在 https://github.com/NVIDIA/cutlass/tree/main/media/docs 中有很好的 doc，主要学习 3.x API(cute)
- cutlass 的自定义算子实践可以参考 flash attention：https://github.com/Dao-AILab/flash-attention
- 需要注意我们目标在 H100 上进行大部分实验，需要学习 H100 的新特性以及对应的编程方法，最好的参考是 PTX 文档 https://docs.nvidia.com/cuda/parallel-thread-execution/index.html ，最好的 Gemm 实践参考 cutlass 的 warpspecialized && tma 系列 gemm，自定义算子参考 flash attention 3：https://github.com/Dao-AILab/flash-attention/tree/main/hopper。推荐 colfax research 的系列 blog 教程：https://research.colfax-intl.com/

# 低比特训练
我们主要做 FP8/INT8 数据格式的训练，这是因为低比特数据有更高的吞吐，可以参考 H100 的性能参数：https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper Table 3
- 我们follow之前INT8块量化的工作 Jetfire：https://github.com/thu-ml/Jetfire-INT8Training/tree/main/Jetfire
- 我们预期对标 nvidia 的 FP8 训练框架 Transformer Engine: https://github.com/NVIDIA/TransformerEngine
这部分内容先了解即可，如果之后的工作是针对具体 GPU 进行优化，优先学习 CUDA

